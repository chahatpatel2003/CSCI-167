{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwjzqpkaH9AoNI04mPciNI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chahatpatel2003/CSCI-167/blob/main/notebook_7_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cojoMTWvmPry",
        "outputId": "667798be-f204-4f68-f64f-d3ab0c9cf37d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True output = 1.907, Your answer = 1.907\n",
            "y = 20.000 Loss = 327.371\n",
            "-----------------------------------------------\n",
            "Bias 0, derivatives from backprop:\n",
            "[[ -4.486]\n",
            " [  4.947]\n",
            " [  6.812]\n",
            " [ -3.883]\n",
            " [-24.935]\n",
            " [  0.   ]]\n",
            "Bias 0, derivatives from finite differences\n",
            "[[ -4.486]\n",
            " [  4.947]\n",
            " [  6.812]\n",
            " [ -3.883]\n",
            " [-24.935]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 1, derivatives from backprop:\n",
            "[[ -0.   ]\n",
            " [-11.297]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-10.722]\n",
            " [  0.   ]]\n",
            "Bias 1, derivatives from finite differences\n",
            "[[  0.   ]\n",
            " [-11.297]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [-10.722]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 2, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [-0.   ]\n",
            " [ 0.938]\n",
            " [ 0.   ]\n",
            " [-9.993]\n",
            " [ 0.508]]\n",
            "Bias 2, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.938]\n",
            " [ 0.   ]\n",
            " [-9.993]\n",
            " [ 0.508]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 3, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [-4.8  ]\n",
            " [-1.661]\n",
            " [-0.   ]\n",
            " [ 3.393]\n",
            " [ 5.391]]\n",
            "Bias 3, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [-4.8  ]\n",
            " [-1.661]\n",
            " [ 0.   ]\n",
            " [ 3.393]\n",
            " [ 5.391]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 4, derivatives from backprop:\n",
            "[[-0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-0.   ]\n",
            " [-5.212]\n",
            " [-0.   ]]\n",
            "Bias 4, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [-5.212]\n",
            " [ 0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 5, derivatives from backprop:\n",
            "[[-36.187]]\n",
            "Bias 5, derivatives from finite differences\n",
            "[[-36.187]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 0, derivatives from backprop:\n",
            "[[ -5.383]\n",
            " [  5.937]\n",
            " [  8.174]\n",
            " [ -4.66 ]\n",
            " [-29.922]\n",
            " [  0.   ]]\n",
            "Weight 0, derivatives from finite differences\n",
            "[[ -5.383]\n",
            " [  5.937]\n",
            " [  8.174]\n",
            " [ -4.66 ]\n",
            " [-29.922]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 1, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 1, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [-32.511  -6.799 -18.282 -34.148 -42.196   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [-30.856  -6.453 -17.352 -32.409 -40.047   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 2, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      5.371   0.      0.      3.145   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
            " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
            "Weight 2, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      5.371   0.      0.      3.145   0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -57.233   0.      0.    -33.506   0.   ]\n",
            " [  0.      2.907   0.      0.      1.702   0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 3, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
            " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      2.597   0.     30.333   7.776]\n",
            " [  0.      0.      4.126   0.     48.188  12.353]]\n",
            "Weight 3, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.     -3.674   0.    -42.905 -10.998]\n",
            " [  0.      0.     -1.272   0.    -14.85   -3.807]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      2.597   0.     30.333   7.776]\n",
            " [  0.      0.      4.126   0.     48.188  12.353]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 4, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Weight 4, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    -81.635 -49.136   0.    -22.007 -10.146]\n",
            " [  0.      0.      0.      0.      0.      0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 5, derivatives from backprop:\n",
            "[[   0.      0.      0.      0.   -400.33    0.  ]]\n",
            "Weight 5, derivatives from finite differences\n",
            "[[   0.      0.      0.      0.   -400.33    0.  ]]\n",
            "Success!  Derivatives match.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-446902071.py:65: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"y = %3.3f Loss = %3.3f\"%(float(y), loss))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set seed so we always get the same random numbers\n",
        "np.random.seed(0)\n",
        "\n",
        "# Number of hidden layers\n",
        "K = 5\n",
        "# Number of neurons per layer\n",
        "D = 6\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "\n",
        "# Make empty lists\n",
        "all_weights = [None] * (K+1)\n",
        "all_biases = [None] * (K+1)\n",
        "\n",
        "# Create input and output layers\n",
        "all_weights[0]  = np.random.normal(size=(D, D_i))\n",
        "all_weights[-1] = np.random.normal(size=(D_o, D))\n",
        "all_biases[0]   = np.random.normal(size=(D,1))\n",
        "all_biases[-1]  = np.random.normal(size =(D_o,1))\n",
        "\n",
        "# Create intermediate layers\n",
        "for layer in range(1, K):\n",
        "  all_weights[layer] = np.random.normal(size=(D, D))\n",
        "  all_biases[layer]  = np.random.normal(size=(D,1))\n",
        "\n",
        "def ReLU(preactivation):\n",
        "  return preactivation.clip(0.0)\n",
        "\n",
        "def compute_network_output(net_input, all_weights, all_biases):\n",
        "  K = len(all_weights) - 1\n",
        "  all_f = [None] * (K+1)\n",
        "  all_h = [None] * (K+1)\n",
        "  all_h[0] = net_input\n",
        "\n",
        "  # Forward through hidden layers 0..K-1\n",
        "  for layer in range(K):\n",
        "      all_f[layer]   = np.matmul(all_weights[layer], all_h[layer]) + all_biases[layer]\n",
        "      all_h[layer+1] = ReLU(all_f[layer])\n",
        "\n",
        "  # Output layer preactivation f_K\n",
        "  all_f[K] = np.matmul(all_weights[K], all_h[K]) + all_biases[K]\n",
        "\n",
        "  net_output = all_f[K]\n",
        "  return net_output, all_f, all_h\n",
        "\n",
        "# Define input\n",
        "net_input = np.ones((D_i,1)) * 1.2\n",
        "# Compute network output\n",
        "net_output, all_f, all_h = compute_network_output(net_input, all_weights, all_biases)\n",
        "print(\"True output = %3.3f, Your answer = %3.3f\"%(1.907, net_output[0,0]))\n",
        "\n",
        "def least_squares_loss(net_output, y):\n",
        "  return np.sum((net_output-y) * (net_output-y))\n",
        "\n",
        "def d_loss_d_output(net_output, y):\n",
        "  return 2*(net_output - y)\n",
        "\n",
        "y = np.ones((D_o,1)) * 20.0\n",
        "loss = least_squares_loss(net_output, y)\n",
        "print(\"y = %3.3f Loss = %3.3f\"%(float(y), loss))\n",
        "\n",
        "def indicator_function(x):\n",
        "  x_in = np.array(x)\n",
        "  x_in[x_in>0]  = 1\n",
        "  x_in[x_in<=0] = 0\n",
        "  return x_in\n",
        "\n",
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "  all_dl_dweights = [None] * (K+1)\n",
        "  all_dl_dbiases  = [None] * (K+1)\n",
        "  all_dl_df       = [None] * (K+1)\n",
        "  all_dl_dh       = [None] * (K+1)\n",
        "\n",
        "  # dL/df_K (output layer)\n",
        "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K], y))\n",
        "\n",
        "  # Backprop through layers K ... 0\n",
        "  for layer in range(K, -1, -1):\n",
        "    # dL/db_layer = dL/df_layer\n",
        "    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n",
        "\n",
        "    # dL/dW_layer = (dL/df_layer) @ h_layer^T\n",
        "    all_dl_dweights[layer] = np.matmul(all_dl_df[layer], all_h[layer].T)\n",
        "\n",
        "    # dL/dh_layer = W_layer^T @ (dL/df_layer)\n",
        "    all_dl_dh[layer] = np.matmul(all_weights[layer].T, all_dl_df[layer])\n",
        "\n",
        "    if layer > 0:\n",
        "      # dL/df_{layer-1} = (dL/dh_layer) âŠ™ ReLU'(f_{layer-1})\n",
        "      all_dl_df[layer-1] = all_dl_dh[layer] * indicator_function(all_f[layer-1])\n",
        "\n",
        "  return all_dl_dweights, all_dl_dbiases\n",
        "\n",
        "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)\n",
        "\n",
        "np.set_printoptions(precision=3)\n",
        "all_dl_dweights_fd = [None] * (K+1)\n",
        "all_dl_dbiases_fd  = [None] * (K+1)\n",
        "delta_fd = 0.000001\n",
        "\n",
        "# Finite-difference check for biases\n",
        "for layer in range(K+1):\n",
        "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
        "  for row in range(all_biases[layer].shape[0]):\n",
        "    all_biases_copy = [np.array(x) for x in all_biases]\n",
        "    all_biases_copy[layer][row] += delta_fd\n",
        "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
        "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2, y))/delta_fd\n",
        "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dbiases[layer])\n",
        "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dbiases_fd[layer])\n",
        "  if np.allclose(all_dl_dbiases_fd[layer], all_dl_dbiases[layer], rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")\n",
        "\n",
        "# Finite-difference check for weights\n",
        "for layer in range(K+1):\n",
        "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
        "  for row in range(all_weights[layer].shape[0]):\n",
        "    for col in range(all_weights[layer].shape[1]):\n",
        "      all_weights_copy = [np.array(x) for x in all_weights]\n",
        "      all_weights_copy[layer][row][col] += delta_fd\n",
        "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
        "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2, y))/delta_fd\n",
        "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dweights[layer])\n",
        "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dweights_fd[layer])\n",
        "  if np.allclose(all_dl_dweights_fd[layer], all_dl_dweights[layer], rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")\n"
      ]
    }
  ]
}