{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVReE/i0xGCTMbQwR3h5al",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chahatpatel2003/CSCI-167/blob/main/notebook_12_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4opPas65Gq3n",
        "outputId": "1bb3dbbb-cf3d-4440-d597-13ea1b7409e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[ 1.78862847],\n",
            "       [ 0.43650985],\n",
            "       [ 0.09649747],\n",
            "       [-1.8634927 ]]), array([[-0.2773882 ],\n",
            "       [-0.35475898],\n",
            "       [-0.08274148],\n",
            "       [-0.62700068]]), array([[-0.04381817],\n",
            "       [-0.47721803],\n",
            "       [-1.31386475],\n",
            "       [ 0.88462238]])]\n",
            "Attentions for output  0\n",
            "[1.24326146e-13 9.98281489e-01 1.71851130e-03]\n",
            "Attentions for output  1\n",
            "[2.79525306e-12 5.85506360e-03 9.94144936e-01]\n",
            "Attentions for output  2\n",
            "[0.00505708 0.00654776 0.98839516]\n",
            "x_prime_0_calculated: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\n",
            "x_prime_1_calculated: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\n",
            "x_prime_2_calculated: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
            "x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\n",
            "[[ 0.94744244  1.64201168  1.61949281]\n",
            " [-0.24348429 -0.08470004 -0.06641533]\n",
            " [-0.91310441  4.02764044  3.96863308]\n",
            " [-0.44522983  2.18690791  2.15858316]]\n",
            "Attention matrix (unscaled):\n",
            "[[1.24326146e-13 2.79525306e-12 5.05707907e-03]\n",
            " [9.98281489e-01 5.85506360e-03 6.54776072e-03]\n",
            " [1.71851130e-03 9.94144936e-01 9.88395160e-01]]\n",
            "[[ 0.97411966  1.59622051  1.32638014]\n",
            " [-0.23738409 -0.09516106  0.13062402]\n",
            " [-0.72333202  3.70194096  3.02371664]\n",
            " [-0.34413007  2.01339538  1.6902419 ]]\n",
            "Attention matrix (scaled):\n",
            "[[3.38843552e-07 1.55730194e-06 6.20418746e-02]\n",
            " [9.60161968e-01 7.12734969e-02 7.05962187e-02]\n",
            " [3.98376935e-02 9.28724946e-01 8.67361907e-01]]\n",
            "Permutation covariance (unscaled): True\n",
            "Permutation covariance (scaled): True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1950077326.py:43: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dot_product = float(key.T @ qn)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(3)\n",
        "N = 3\n",
        "D = 4\n",
        "all_x = []\n",
        "for n in range(N):\n",
        "  all_x.append(np.random.normal(size=(D,1)))\n",
        "print(all_x)\n",
        "\n",
        "np.random.seed(0)\n",
        "omega_q = np.random.normal(size=(D,D))\n",
        "omega_k = np.random.normal(size=(D,D))\n",
        "omega_v = np.random.normal(size=(D,D))\n",
        "beta_q = np.random.normal(size=(D,1))\n",
        "beta_k = np.random.normal(size=(D,1))\n",
        "beta_v = np.random.normal(size=(D,1))\n",
        "\n",
        "all_queries = []\n",
        "all_keys = []\n",
        "all_values = []\n",
        "for x in all_x:\n",
        "  query = omega_q @ x + beta_q\n",
        "  key = omega_k @ x + beta_k\n",
        "  value = omega_v @ x + beta_v\n",
        "  all_queries.append(query)\n",
        "  all_keys.append(key)\n",
        "  all_values.append(value)\n",
        "\n",
        "def softmax(items_in):\n",
        "  a = np.array(items_in, dtype=float).reshape(-1,1)\n",
        "  a = a - np.max(a)\n",
        "  e = np.exp(a)\n",
        "  out = (e / np.sum(e)).reshape(-1)\n",
        "  return out\n",
        "\n",
        "all_x_prime = []\n",
        "for n in range(N):\n",
        "  all_km_qn = []\n",
        "  qn = all_queries[n]\n",
        "  for key in all_keys:\n",
        "    dot_product = float(key.T @ qn)\n",
        "    all_km_qn.append(dot_product)\n",
        "  attention = softmax(all_km_qn)\n",
        "  print(\"Attentions for output \", n)\n",
        "  print(attention)\n",
        "  x_prime = np.zeros((D,1))\n",
        "  for i, v in enumerate(all_values):\n",
        "    x_prime += attention[i] * v\n",
        "  all_x_prime.append(x_prime)\n",
        "\n",
        "print(\"x_prime_0_calculated:\", all_x_prime[0].transpose())\n",
        "print(\"x_prime_0_true: [[ 0.94744244 -0.24348429 -0.91310441 -0.44522983]]\")\n",
        "print(\"x_prime_1_calculated:\", all_x_prime[1].transpose())\n",
        "print(\"x_prime_1_true: [[ 1.64201168 -0.08470004  4.02764044  2.18690791]]\")\n",
        "print(\"x_prime_2_calculated:\", all_x_prime[2].transpose())\n",
        "print(\"x_prime_2_true: [[ 1.61949281 -0.06641533  3.96863308  2.15858316]]\")\n",
        "\n",
        "def softmax_cols(data_in):\n",
        "  m = np.max(data_in, axis=0, keepdims=True)\n",
        "  exp_values = np.exp(data_in - m)\n",
        "  denom = np.sum(exp_values, axis=0, keepdims=True)\n",
        "  softmax = exp_values / denom\n",
        "  return softmax\n",
        "\n",
        "def self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "  Q = omega_q @ X + beta_q\n",
        "  K = omega_k @ X + beta_k\n",
        "  V = omega_v @ X + beta_v\n",
        "  S = K.T @ Q\n",
        "  A = softmax_cols(S)\n",
        "  X_prime = V @ A\n",
        "  return X_prime, A\n",
        "\n",
        "X = np.zeros((D, N))\n",
        "X[:,0] = np.squeeze(all_x[0])\n",
        "X[:,1] = np.squeeze(all_x[1])\n",
        "X[:,2] = np.squeeze(all_x[2])\n",
        "\n",
        "X_prime, A = self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "print(X_prime)\n",
        "print(\"Attention matrix (unscaled):\")\n",
        "print(A)\n",
        "\n",
        "def scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k):\n",
        "  Q = omega_q @ X + beta_q\n",
        "  K = omega_k @ X + beta_k\n",
        "  V = omega_v @ X + beta_v\n",
        "  S = (K.T @ Q) / np.sqrt(X.shape[0])\n",
        "  A = softmax_cols(S)\n",
        "  X_prime = V @ A\n",
        "  return X_prime, A\n",
        "\n",
        "X_prime_scaled, A_scaled = scaled_dot_product_self_attention(X,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "print(X_prime_scaled)\n",
        "print(\"Attention matrix (scaled):\")\n",
        "print(A_scaled)\n",
        "\n",
        "perm = np.array([2,0,1])\n",
        "P = np.eye(N)[:,perm]\n",
        "X_perm = X @ P\n",
        "Xp1, _ = self_attention(X_perm,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "Xp2, _ = scaled_dot_product_self_attention(X_perm,omega_v, omega_q, omega_k, beta_v, beta_q, beta_k)\n",
        "print(\"Permutation covariance (unscaled):\", np.allclose(Xp1, X_prime @ P, atol=1e-8))\n",
        "print(\"Permutation covariance (scaled):\", np.allclose(Xp2, X_prime_scaled @ P, atol=1e-8))\n"
      ]
    }
  ]
}